<!DOCTYPE html>
<html lang="en-us">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta name="generator" content="Hugo 0.58.3" />
  
  
  
  <title>
    
    Deep Learning Notes Part 1 | Martin Dickson
    
  </title>
  <link rel="canonical" href="https://mjdickson.dev/posts/goodfellow-deep-learning-pt1/">
  
  
  
  
  
  <link rel="stylesheet" href="https://mjdickson.dev/css/base.min.1a54fb9deb4432a6bdbfaae514a30a7d9b401dbdb840a36b206dfd5d3ee60557.css" integrity="sha256-GlT7netEMqa9v6rlFKMKfZtAHb24QKNrIG39XT7mBVc=" crossorigin="anonymous">
  
  
</head>
<body>
  <nav class="u-background">
  <div class="u-wrapper">
    <ul class="Banner">
      <li class="Banner-item Banner-item--title">
        <a class="Banner-link u-clickable" href="https://mjdickson.dev/">Martin Dickson</a>
      </li>
      
      <li class="Banner-item">
        <a class="Banner-link u-clickable" href="https://mjdickson.dev/posts/">Posts</a>
      </li>
      
      <li class="Banner-item">
        <a class="Banner-link u-clickable" href="https://mjdickson.dev/research/">Research</a>
      </li>
      
      <li class="Banner-item">
        <a class="Banner-link u-clickable" href="https://mjdickson.dev/about/">About</a>
      </li>
      
    </ul>
  </div>
</nav>

  <main>
    <div class="u-wrapper">
      <div class="u-padding">
        

<article>
  <script type="text/x-mathjax-config">MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$']]}});</script>
<script type="text/javascript" async src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML"></script>

<header class="Heading">
  <h2 class="Heading-title">
    <a class="Heading-link u-clickable" href="https://mjdickson.dev/posts/goodfellow-deep-learning-pt1/" rel="bookmark">Deep Learning Notes Part 1</a>
  </h2>
  
  <time datetime="2019-09-24T08:08:00&#43;01:00">
    24 September, 2019
  </time>
  
</header>

  

<p>I&rsquo;ve been reading the book <a href="https://www.deeplearningbook.org/">Deep Learning (Goodfellow et. al)</a> recently.  I&rsquo;m doing this with a view to filling in gaps about my deep learning knowledge, as most of my learning in the past has been from random blogs posts and the occassional research paper.</p>

<p>Since this is a fairly long book and I am constrained in the amount of time I can dedicate to reading it, I&rsquo;ve tried an experiment with keeping quick markdown notes of what I&rsquo;ve read.  This means that when I come back to reading after a short break, I can remember the salient points of where I was by a quick skim over my notes.</p>

<p>In the unlikely situation that my notes could be useful to anyone else, I though I&rsquo;d post them here.  Due to my background, I skipped the mathematical prelude before Chapter 5.  This posts covers the first half of my reading, Chapters 5-8, which is roughly the &ldquo;groundwork&rdquo; before we look at some specific kinds of neural nets (CNNs and RNNs) and research applications.</p>

<h2 id="chapter-5-machine-learning-basics">Chapter 5 - Machine Learning Basics</h2>

<h3 id="learning-algorithms">Learning Algorithms</h3>

<ul>
<li>A computer program is said to learn from experience $E$ to do task $T$, measured by performance measure $P$, if its performance at $T$, as measured by $P$, is improved with $E$</li>
<li>Some examples of tasks you might want to perform:

<ul>
<li>Classification: learn a function $f: \mathbb{R}^n \to {1,&hellip;,k}$, or output a probability distribution over the classes,</li>
<li>Regression: predict $f: \mathbb{R}^n \to \mathbb{R}$, such as predicting a house price</li>
<li>Transcription (OCR, speech recognition), machine translation, anomaly detection, estimating pdfs, &hellip;</li>
</ul></li>
<li>E.g. performance measure: for classification, could measure performance by accuracy (ratio of correct/all) or error rate (incorrect/all).  Can think of error rate as expected 0-1 loss (lose 0 if correct, lose 1 if incorrect).</li>
<li>Supervised vs unsupervised is a differentation between the kind of experience the process is allowed

<ul>
<li>In a technical sense, there isn&rsquo;t a huge difference.  In supervised learning, you&rsquo;re definitely learning $p(y|\mathbf{x})$.  In unsupervised, you&rsquo;re arguably learning $p(\mathbf{x})$.  By ordering the features of $\mathbf{x}$ and succcessively conditioning on them, the latter reduces to the former.</li>
<li>In practice, what is computationally tractable is presumably very different</li>
</ul></li>
<li>Reinforcement learning is out of scope for this book</li>
<li>Details of simplest example of machine learning (linear regression):

<ul>
<li>We are learning a linear function $f: \mathbb{R}^n \to \mathbb{R}$

<ul>
<li>Task $T$ is to predict $y$ from $\mathbf{x}$ by outputting $y = \mathbf{w}^{\intercal} \mathbf{x}$</li>
<li>Experience $E$ will be whatever dataset we give it, i.e. the pairs $(\mathbf{x}, y)$</li>
<li>Performance measure $P$ is to what extent it minimises the MSE loss function on the test data set</li>
</ul></li>
<li>Intuitively, it makes sense to choose the weight $\mathbb{w}$ that minimise the MSE on the training dataset.  This can be found with basic calculus.  (These are the <em>normal equations</em>)</li>
<li>Now we have a $\mathbb{w}$ and can make predictions.  They&rsquo;re probably reasonably good.</li>
</ul></li>
</ul>

<h3 id="capacity-overfitting-and-underfitting">Capacity, Overfitting and Underfitting</h3>

<ul>
<li>Minimising the training error in itself (like linear regression above) is just an optimisation problem</li>
<li>Generalisation/test error is the expected error on a new input, with expectation being taken across possible inputs.  This is what we really care about

<ul>
<li>Obviously this is hard to compute in practice, but it can be estimated by averaging the error on a reasonable number of test examples</li>
</ul></li>
<li>Statistical learning theory comes in to play:

<ul>
<li>For the algorithm to have any success, obviously there has to be some relation between the training set and the test set</li>
<li>Typically, examples are assumed iid, and training and test examples are drawn from the same population</li>
<li>The underlying distribution is called the data-generating distribution, $p_{\text{data}}$</li>
<li>Since they are drawn from the same data-generating distribution, clearly if we fix the parameters and calculate the expected training and test errors, they are the same</li>
</ul></li>
<li>In practice, you set the parameters only after examining the training set, then compute the expected test error.  So the expected test error is $&gt;=$ to expected training error</li>
<li><em>Underfitting</em>: when the model can&rsquo;t get a low error on the training set</li>
<li><em>Overfitting</em>: when the test error is much larger than training error</li>
<li>Since expected test error $&gt;=$ expected training error, both underfitting and overfitting mean bad (expected) test errors</li>
<li><em>Model capacity</em>: think of large capacity = high degree polynomial fit.  Large capacity tends to overfit, low capacity tends to underfit</li>
<li><em>Representational capacity</em>: the family of functions the learning algorithm can choose (e.g. in my setup, maybe I&rsquo;m fitting a polynomial and any polynomial of deg $&lt;=10$ is fair game)</li>
<li><em>Effective capacity</em>: what the learning algorithm can actually reach inside the representational capacity, given the shortcomings of the optimisation algorithm (e.g., walking in the space of polynomials of degree $&lt;=10$ with discrete step sizes =&gt; not all real coefficients are possible =&gt; only a subset of polynomials are actually reachable)</li>
<li><em>Vapnik-Chervonenkis dimension</em>: measures the capacity of a binary classifier.

<ul>
<li>Defined as largest $m$ s.t. there exists a training set of size $m$ different points that the classifier can label arbitrarily.</li>
<li>So for example a classifier which just assigns the same label to all samples has a VC dimension of $0$.</li>
</ul></li>
<li><em>Statistical learning theory</em>: proves theorems like $(\text{test error}) &lt;= (\text{training error}) \times f(\text{capacity}, \text{# training samples})$, where $f$ increases as capacity does, but decreases as # training samples does.

<ul>
<li>In practice, have no idea what the capacity of deep learning algorithms are.</li>
<li>The optimisation algorithm affects the capacity (effective capacity vs. representational capacity), and we don&rsquo;t understand the optimisation algorithm</li>
</ul></li>
<li><em>Parametric models</em>: learn a function described by a parameter vector whose size is finite and fixed before any data is observed

<ul>
<li>E.g. linear regression is a parametric model - there is a fixed number of weights before starting</li>
</ul></li>
<li>Non-parametrics models are everything else

<ul>
<li>E.g. nearest neighbour regression: given a test sample $\mathbf{x}$, I search through all my training samples $\mathbf{X}$ and find the one closest to $\mathbf{x}$, and return the output $y$ for that closest training sample.</li>
<li>Could also allow the parameter list to grow inside a parametric model, e.g. allow unbounded polynomial expansions of input to linear regression</li>
</ul></li>
<li><em>Bayes error</em>: error of a classifier which knows the true joint probability distrubtion of $(\mathbf{x}, y)$.  If $\mathbf{x} \to y$ is deterministic then Bayes error is zero, but otherwise it will be non-zero.</li>
<li>No free lunch theorem: averaged over all possible data-generating distributions, every classification algorithm has the same error rate when classifying previously unobserved points

<ul>
<li>I.e. there is no &ldquo;universally good&rdquo; classifier</li>
<li>I.e. need to choose the right classifer for the right problem</li>
</ul></li>
<li>Regularisation: roughly, making the learning algorithm prefer one solution over another

<ul>
<li>E.g. weight decay in linear regression.  Instead of just minimising the MSE on the training set, we now minimise $J(\mathbf{w}) = \text{MSE}_{\text{train}} + \lambda \mathbf{w}^{\intercal} \mathbf{w}$.  When $\lambda = 0$ this is the same as before, but when $\lambda$ is large it forces us to prefer $\mathbf{w}$ whose coefficients are small</li>
<li>Previously we flat-out excluded certain functions from the hypothesis space of functions.  In a sense, this is expressing an infinitely strong preference against that function.  Regularisation more generally gives a more nuanced approach</li>
</ul></li>
<li>Regularisation: actual definition given, any modification we make to a learning algorithm that is intended to reduce its generalisation error but not its training error</li>
</ul>

<h3 id="hyperparameters-and-validation-sets">Hyperparameters and Validation sets</h3>

<ul>
<li>Hyperparameters: settings we use to control an algorithm&rsquo;s behaviour

<ul>
<li>E.g. degree of polynomial in linear regression, value of the weight decay parameter $\lambda$</li>
</ul></li>
<li>Could deem any parameter a hyperparameter (and hence stop the learning algorithm from to optimise it)

<ul>
<li>Maybe because some parameter is difficult to optimise</li>
<li>Maybe because it doesn&rsquo;t make sense to attempt to learn that parameter in the current problem.  E.g. don&rsquo;t try to learn $\lambda$ in your linear regression problem with large capacity, because it&rsquo;s definitely going to overfit by picking $\lambda=0$ and a high degree polynomial</li>
</ul></li>
<li>Use a validation set for checking hyperparameters.  We still keep the test set separate (model, including its hyerparameters, should know nothing about the test set).  Instead, we split the training set in to two parts.  Fix the hyperparameters and use the first part to learn the parameters.  Then use the second part to estimate the generalisation error.  If it looks terrible, go back and tweak the hyperparameters.  If it looks good, proceed to the test set.

<ul>
<li>Somewhat confusing, the &ldquo;first part of the training set&rdquo; is called the training set as well.</li>
<li>Rule of thumb: roughly 80% of the training data goes to actual training, roughly 20% goes to validation</li>
</ul></li>
<li>If the dataset available to you is small, dividing in to a fixed training and test set is problematic.  With little data, there is lots of statistical uncertainty.</li>
<li>Cross-validation gets round this, at cost of increased computation.

<ul>
<li>E.g. $k$-fold cross-validation.  Split the dataset in to $k$ parts.  Do $k$ passes through the data.  On the $i$th pass, declare the $i$th part to be the test set and the rest to be the training set.</li>
<li>Can do the same thing with training/validation/test splits</li>
<li>Cross-validation is potentially problematic, <a href="http://www.jmlr.org/papers/volume5/grandvalet04a/grandvalet04a.pdf">Bengio and Grandvalet (2004)</a> probably worth reading</li>
</ul></li>
</ul>

<h3 id="estimators-bias-and-variance">Estimators, Bias, and Variance</h3>

<ul>
<li>Basic stats things like parameter estimation, bias, and variance are used in ML to talk about generalization, underfitting, and overfitting</li>
<li>Point estimation: supposed to capture notion of providing a single &ldquo;best&rdquo; prediction of a quantity of interest</li>
<li>Given $m$ iid data points $\mathbf{x}^{(1)}, &hellip;, \mathbf{x}^{(m)}$, define a <em>point estimate</em> (or <em>statistic</em>) as any function of the data $\widehat{\theta}_{m} = g(\mathbf{x}^{(1)}, &hellip;, \mathbf{x}^{(m)})$.

<ul>
<li>This definition is literally any function of the data, doesn&rsquo;t have to be related to anything</li>
<li>Obviously in practice we are interested in functions for which $\widehat{\theta}_m$ is close to some quantity of interest $\theta$</li>
</ul></li>
<li>Take the frequentist approach: assume the true value of $\theta$ is fixed but unknown.  Note that since $\widehat{\theta}$ is a function of the data, which is drawn from a probability distribution, $\widehat{\theta}$ is itself a random variable</li>
<li>Function estimation: estimating a function of the data.  This is just a point estimate in a function space, e.g. linear regression is estimating a point in the space spanned by the coefficients of the weight vector</li>
<li>Bias of an estimator: $\text{bias}(\widehat{\theta}_m) = \mathbb{E}(\widehat{\theta}_m) - \theta$. Here the expectation is over the data $\mathbf{x}^{(1)},&hellip;\mathbf{x}^{(m)}$, and $\theta$ is the true value

<ul>
<li>Say $\widehat{\theta}_m$ is unbiased if $\text{bias}(\widehat{\theta}_m) = 0$</li>
<li>Say $(\widehat{\theta}_m)_{m \geq 1}$ is asymptotically unbiased if $\lim_{m \rightarrow \infty} \text{bias}(\widehat{\theta}_m) = 0$</li>
</ul></li>
<li>E.g. Bernoulli distribution with mean $\theta$ (so $P(x; \theta) = \theta^{x} (1-\theta)^{1-x}$).

<ul>
<li>Given observations $x^{(1)},&hellip;,x^{(m)}$, we might make the point estimate $\widehat{\theta}_m = \frac{1}{m} \sum_{i=1}^m x^{(i)}$</li>
<li>Simple calculation shows this is an unbiased estimate of $\theta$</li>
</ul></li>
<li>E.g. Gaussian $\mathcal{N}(x; \mu, \sigma)$

<ul>
<li>Again with $m$ observations, try the sample mean $\widehat{\mu}_m = \frac{1}{m} \sum_{i=1}^m x^{(i)}$ as an estimator for the mean</li>
<li>Very similar computation shows that this is unbiased</li>
<li>Now try sample variance $\widehat{\sigma}_m^2 = \frac{1}{m} \sum_{i=1}^m \left(x^{(i)} - \widehat{\mu}_m \right)^2$ as an estimator for the variance</li>
<li>Standard calculation shows $\text{bias}(\widehat{\sigma}_m^2) = -\sigma^2/m$, so this is a biased estimator</li>
<li>Unbiased sample variance is $\tilde{\sigma}_m^2 = \frac{1}{m-1} \sum_{i=1}^m \left(x^{(i)} - \widehat{\mu}_m \right)^2$</li>
<li>This one is unbiased</li>
</ul></li>
<li>We computed the expectation of an estimator to measure its bias, we can also compute the variance of an estimator.  Intuitively, we&rsquo;d like this variance to be low</li>
<li>Write $\text{Var}(\widehat{\theta})$ for the variance (variance over the empirical distribution again), and write $\text{SE}(\widehat{\theta})$ for the standard error (positive square root of the variance)</li>
<li>E.g. Standard error of sample mean is $\text{SE}(\widehat{\mu}_m) = \sqrt{\text{Var}(\frac{1}{m}\sum_{i=1}^m x^{(i)})} = \sigma/\sqrt{m}$, where $\sigma$ is the true standard error.

<ul>
<li>We don&rsquo;t <em>know</em> $\sigma$ though.  We can attempt to get an <em>estimate</em> for the standard error of the sample mean by using an estimate for $\sigma$</li>
<li>The two we&rsquo;ve seen (square root of sample variance, square root of unbiased sample variance) - both result in biased estimates</li>
<li>The square root of the unbiased sample variance is used in practice, it works okay for large $m$</li>
</ul></li>
<li>Use of standard error in ML:<br />

<ul>
<li>Have some model capable of making predictions.  Look at some of the errors it makes on the test set, and compute the sample mean.<br /></li>
<li>This is an estimate of the true error on the test set, i.e. the generalisation error.  How close this is to the true error depends on size of test set</li>
<li>By the central limit theorem, our sample mean is normally distributed around the true error</li>
<li>We don&rsquo;t know the standard error of this normal, but we can use an <em>estimate</em> for the standard error, as above.  Hence we can compute confidence intervals for the true expected error</li>
<li>E.g. 95% confidence interval centred on $\widehat{\mu}_m$ is $(\widehat{\mu}_m - 1.96\text{SE}(\widehat{\mu}_m), \widehat{\mu}_m  + 1.96\text{SE}(\widehat{\mu}_m))$ under the normal distribution with mean $\widehat{\mu}_m$ and variance $\text{SE}(\widehat{\mu}_m)^2$</li>
</ul></li>
<li>E.g. Bernoulli distribution

<ul>
<li>Have sample mean $\widehat{\theta}_m$, compute its variance $\text{Var}(\widehat{\theta}_m)$</li>
<li>Turns out $\text{Var}(\widehat{\theta}_m) = \frac{\theta(1-\theta)}{m}$</li>
<li>As $m \rightarrow \infty$, $\text{Var}(\widehat{\theta}_m) \rightarrow 0$.  This is quite common</li>
</ul></li>
<li>Intuitively, if you are learning a function and you overfit, you would expect your estimate for that function to have low bias but high variance</li>
<li>Similarly, if you underfit, you would expect your estimate for the function to have low variance but potentially high bias</li>
<li>Overall quality of an estimate takes both into account.  E.g. the MSE satisfies $\mathbb{E}[(\widehat{\theta}_m - \theta)^2] = \text{bias}(\theta_m) + \text{Var}(\theta_m)$.  There is probably a sweet spot where both bias and variance are reasonably small that gives the best error in the MSE sense</li>
<li>Taking an increasing number $m$ of samples from the training set, our estimate $\widehat{\theta}$ is actually a sequence $(\widehat{\theta})_m$.  We say $\widehat{\theta}$ is <em>weakly consistent</em> if the sequence converges in probability to $\theta$, and is <em>strongly consistent</em> if the sequence converges almost surely to $\theta$.

<ul>
<li>I guess we can construct many sequences from one estimator, but they&rsquo;ll converge in probability to each other anyway so it doesn&rsquo;t matter?  Might need some reasonability assumptions on $\widehat{\theta}$</li>
</ul></li>
<li>Clearly (weakly) consistent estimators are asymptotically unbiased</li>
<li>Converse is false, e.g. estimate the mean of a Gaussian by always saying the mean is always the first value you saw, no matter how many datapoints are thrown at you.  This fits the definition of an estimator, is unbiased (hence asymptotically unbiased), but is not consistent</li>
</ul>

<h3 id="maximum-likelihood-estimation">Maximum Likelihood Estimation</h3>

<ul>
<li>Random guessing is probably not a good way to build estimators in general.  MLE is the most common systematic way to do so</li>
<li>Have $m$ samples $\mathbf{x}^{(i)}$ drawn independently from some (unknown) distribution $p_{\text{data}}$.  Have some parameter $\theta$ and a family of distributions $p(\mathbf{x}; \theta)$ s.t. for each concrete value of $\theta$ we get some distribution $p(\mathbf{x})$ which may or may not be related to $p_\text{data}(\mathbf{x})$.  The MLE for $\theta$ is $\theta_{\text{ML}} = \text{argmax}_{\theta} \prod_{i=1}^m p(\mathbf{x}^{(i)}; \theta)$, i.e. the choice of $\theta$ that maximises the likelihood of the observed data</li>
<li>More numerically convenient to work with log-likelihood, $\theta_\text{ML} = \text{argmax}_\theta \sum_{i=1}^m \log p(\mathbf{x}^{(i)}; \theta)$.  Equivalently, $\theta_\text{ML} = \text{argmax}_\theta \mathbb{E}_{\mathbf{x} \sim \widehat{p}_{\text{data}}} [\log p(\mathbf{x}; \theta)]$, where $\widehat{p}_\text{data}$ is the empirical distribution coming from our sampling from $p_\text{data}$</li>
<li>E.g. suppose I have $m$ samples from a Bernoulli distribution with parameter $\theta$, but I don&rsquo;t know $\theta$ and I want to estimate it instead.  For a particular value of $\theta$, I know that $p(x; \theta) = \theta^{x}(1-\theta)^{1-x}$.  So the thing I&rsquo;m trying to argmax is $\sum_{i=1}^m \log (\theta^{x_m}(1-\theta)^{1-x_m}) = \sum_{i=1}^m x_m \log(\theta) + (1-x_m)\log(1-\theta)$.  The derivative of this is $\sum_{i=1}^m \frac{x_m}{\theta} - \frac{1-x_m}{1-\theta}$ which is extremal when $\sum_{i=1}^m(1-\theta)x_m - (1-x_m)\theta = 0$, i.e. $m\theta_{\text{ML}} = \sum_{i=1}^m x_m$, i.e. at the sample mean.  This can be checked to be a maximum.</li>
<li>Equivalent point of view: MLE is the estimate that moves $p(\mathbf{x}; \theta)$ closest to the empirical distribution $\widehat{p}_{\text{data}}$ in the KL sense.  (A short calculation shows that attempting to minimise the KL divergence boils down to maximising the above expectation over the empirical distribution.)<br /></li>
<li>In a supervised setting, can do the same thing with conditional probabilities.  Here $\theta_{\text{ML}} = \text{argmax}_{\theta} \sum_{i=1}^m \log p(\mathbf{y}^{(i)} | \mathbf{x}^{(i)}; \theta)$</li>
<li>E.g. linear regression.  Previously we just searched for functions of the form $y = \mathbf{w}^T \mathbf{x}$, and decided to pick $\mathbf{w}$ that minimised the mean square error on the training set.  Now think of the linear regression as outputting a conditional distribution $p(y | \mathbf{x})$ with Gaussian noise, i.e. we define $p(y|\mathbf{x}) = \mathcal{N}(y; \widehat{y}(\mathbf{x}; \mathbf{w}), \sigma^2)$.  A short calculation shows that maximises the log-likelihood of this is equivalent to minimising the the MSE on the training set as before</li>
<li>MLE is consistent, under the condition that there is precisely one value of $\theta$ s.t. $p_{\text{data}} = p(.; \theta)$</li>
<li>Consistency says nothing about the rate of convergence.  That is measured by statistical efficiency, which is quantified for parameter estimates by the MSE (over the data-generating distribution) between the estimated and the true parameter.  Cramer&ndash;Rao says that the MLE has the lowest MSE amongst all consistent estimators, so in this sense MLE is the asymptotically best estimator</li>
<li>In practice, with small amounts of training data, maybe use regularization strategies to get biased versions of MLE which have lower variance</li>
</ul>

<h3 id="bayesian-statistics">Bayesian Statistics</h3>

<ul>
<li>Frequentist approach: true value of $\theta$ is fixed but unknown, the point estimate $\widehat{\theta}$ is a random variable (since it is a function of the dataset, which is regarded as random)</li>
<li>Bayesian approach: dataset is directly observed (so not random), and $\theta$ is unknown <em>and uncertain</em> so $\theta$ itself is a random variable</li>
<li>Before observing data, represent our knowledge of $\theta$ by specifying a prior distribution $p(\theta)$.  Initially it can just be some distribution with high entropy, e.g. uniformly distributed in some range, or Gaussian</li>
<li>Given data $x^{(1)},&hellip;,x^{(m)}$, we have the posterior distribution $p(\theta|x^{(1)},&hellip;,x^{(m)}) = \frac{p(x^{(1)},&hellip;,x^{(m)}|\theta)p(\theta)}{p(x^{(1)},&hellip;,x^{(m)})}$</li>
<li>E.g. Bayesian linear regression:

<ul>
<li>In standard linear regression, you have $\widehat{\mathbf{y}} = \mathbf{X}\mathbf{w}$, where $\mathbf{X}$ is the whole training set.<br /></li>
<li>First we found optimal $\mathbf{w}$ by minimising the MSE.<br /></li>
<li>Then we redid it by assuming $p(y|\mathbf{x})$ was Gaussian (say with $\sigma^2=1$, standard apparently) and using the MLE for $\mathbf{w}$.  This recovered the same value as MSE.</li>
<li>For Bayesian, we still work with $p(\mathbf{y}|\mathbf{X}, \mathbf{w}) = \mathcal{N}(\mathbf{y}|\mathbf{X}\mathbf{w}; \mathbf{I})$.  We choose a prior $p(\mathbf{w}) = \mathcal{N}(\mathbf{w}; \mathbf{\mu}_0, \mathbf{\Lambda}_0)$.  Computing the posterior distribution $p(\mathbf{w}|\mathbf{X},\mathbf{y})$, turns out to be Gaussian again, with the mean and covariance matrix a function of both the priors and the data (i.e. Gaussian family is <em>self conjugate</em>)</li>
</ul></li>
<li>In the Bayesian approach, you can make predictions using the whole posterior distribution of $\theta$.  If it&rsquo;s computationally intractible, though, you might want a point estimate of $\theta$ anyway to use for your predictions.  This estimate will obviously take your prior into account, unlike a purely frequentist approach</li>
<li>This is where MAP estimates come in, $\theta_{\text{MAP}} = \text{argmax}_{\theta} p(\theta | \mathbf{x}) = \text{argmax}_{\theta} \left(\log(p(x|\theta)) + \log(p(\theta))\right)$.  This is MLE with regularisation</li>
<li>E.g. Bayesian linear regression with $\mathcal{N}(\mathbf{w}; 0, \frac{1}{\lambda}\mathbf{I})$ prior.  The log prior term then corresponds to weight decay</li>
</ul>

<h3 id="supervised-learning-algorithms">Supervised Learning Algorithms</h3>

<ul>
<li>Logisitic regression: force a linear function $\mathbf{x} \mapsto \mathbf{\theta}^T \mathbf{x}$ to take values in $(0,1)$ by composing with the logistic sigmoid $\sigma(x) = 1/(1+e^{-x})$, and interpret the result as a probability.  Now you can use a linear function to do classification: $p(y=1|\mathbf{x},\mathbf{\theta}) = \sigma(\mathbf{\theta}^T\mathbf{x})$.  No closed form solution for the optimal weights, instead search using gradient descent</li>
<li>Support vector machines:

<ul>
<li>Have $f(\mathbf{x}) = b + \sum_i \alpha_i k(\phi(\mathbf{x}), \phi(\mathbf{x}^{(i)}))$, where $\phi$ is some feature mapping and $k$ is some kernel.  Simplest case would be $\phi$ is the identity and $k$ is the dot product, but probably we&rsquo;re thinking of $\phi$ mapping in to some infinite-dimensional space</li>
<li>Most commonly used kernel is the Gaussian kernel, $k(\mathbf{u}, \mathbf{v}) = \mathcal{N}(\mathbf{u}-\mathbf{v}; 0, \sigma^2 \mathbf{I})$, corresponds to a dot-product in an infinite dimensional space</li>
<li>Think of Gaussian kernel as performing template matching.  Training example $(\mathbf{x}, y)$ becomes a template for class $y$.  When $\mathbf{x}&lsquo;$ is near $\mathbf{x}$ in the Euclidean sense, the Gaussian kernel has a large response, so the contribution of $\mathbf{x}$ to the prediction for $\mathbf{x}&lsquo;$ will be large (the corresponding $\alpha$ will count a lot).  We combine those for all training examples $\mathbf{x}$ to get our final prediction for $\mathbf{x}&lsquo;$</li>
<li>Generally, algorithms that look like this are called <em>kernel machines</em> or <em>kernel methods</em></li>
<li>Since $f(\mathbf{x})$ involves each $\alpha_i k(\mathbf{x}, \mathbf{x}^{(i)})$ this could be very expensive to evaluate (involves computing the kernel once for each training sample).  In SVMs, most the of the $\alpha_i$ are zero, so it&rsquo;s not so bad.  Those $\alpha_i$ that are non-zero are called <em>support vectors</em></li>
<li>History note: current deep learning renaissance <a href="https://www.cs.toronto.edu/~hinton/absps/fastnc.pdf">began</a> when a neural network was shown to outperform the radial basis function kernel SVM on MNIST</li>
</ul></li>
<li>$k$-nearest-neighbours (kNN) is sort of a supervised learning algorithm.  But there&rsquo;s not really much learning, it&rsquo;s really just a function of the training data (given the thing you&rsquo;re predicting, find the $k$ nearest neighbours and average their votes)

<ul>
<li>This is a non-parametric learning algorithm, so it has a very high capacity.  So (at a high computational cost) it can obtain a high accuracy on a large training set.  It might generalize badly from a small training set (overfitting)</li>
<li>Disadvantage of naive kNN is that it treats all features equally, so if you had 100 features but only the first one actually mattered for the prediction, kNN wouldn&rsquo;t know that.  Of course you could alter the &ldquo;nearest&rdquo; metric to emphasise the first component, but I guess if you already knew that the first one was the only one that mattered you wouldn&rsquo;t be training a model with all 100 features</li>
</ul></li>
<li>Decision trees, divide space into subregions, subregions are in bijection with leaf nodes.

<ul>
<li>Use specific algorithms to learn the tree, don&rsquo;t really care about any of that in this book</li>
<li>Standard setup has axis aligned splits (e.g. $x_2 &gt; 5$), so will struggle to approximate conditions like $x_2 &gt; x_1$</li>
<li>Seems pretty down on decision trees :)</li>
</ul></li>
</ul>

<h3 id="unsupervised-learning-algorithms">Unsupervised Learning Algorithms</h3>

<ul>
<li>Line between supervised and unsupervised is blurry - subjective to say whether a value is a feature or is provided by a supervisor</li>
<li>In practice unsupervised normally means working with data that has not been manually annotated by a human.  Normally involves density estimation, learning to draw samples from a distribution, learning to denoise data from a distribution, finding a manifold that data lies near/on, or clustering the data</li>
<li>Classic unsupervised learning problem is finding a lower-dimensional representation of the data.  Or a sparse representation.  Or an independent representation (i.e. aim for representation where features are statistically independent)</li>
<li>Principle Component Analysis (PCA):

<ul>
<li>PCA provides a means of compressing data.  Can also view as an unsupervised learning algorithm learning a representation of the data, specifically a lower-dimensional representation whose dimensions have no linear correlation with each other</li>
<li>Recap of algorithm: have points $\mathbf{x} \in \mathbf{R}^n$, want lower dimensional representations $\mathbf{z} \in \mathbf{R}^l$.  Consider linear transformations $D:\mathbf{R}^l \to \mathbf{R}^n$ s.t. the columns of $D$ are orthonormal, i.e. $D$ is the decoding function.  The encoding function is just $\mathbf{x} \mapsto D^{\intercal} \mathbf{x}$, it can be checked this function minimises the distance between a given point $\mathbf{x}$ and its reconstruction.  $D$ is chosen by minimising the cumulative distance between <em>all</em> points and their reconstruction.  After a fair amount of maths we find that $D$ is matrix whose columns are the $l$ eigenvectors of $\mathbf{X}^{\intercal} \mathbf{X}$ corresponding to the largest eigenvalues</li>
<li>How does PCA decorrelate the input $\mathbf{X}$?

<ul>
<li>WLOG assume the data has mean zero, $m$ samples of dimension $n$.  The unbiased sample covariance is $\text{Var}(\mathbf{x}) = \frac{1}{m-1}\mathbf{X}^T \mathbf{X}$.  PCA finds a representation $\mathbf{z} = \mathbf{W}^T \mathbf{x}$ where $\text{Var}(\mathbf{z})$ is diagonal</li>
<li>Basically, we find a rotation $\mathbf{W}$ of input space that aligns the principle axes of variance with the basis of the rotated input space</li>
</ul></li>
</ul></li>
<li>$k$-means clusters:

<ul>
<li>Divides the training set in to $k$ different clusters of examples that are near each other, so it&rsquo;s like a $k$-dimensional one-hot coding of the input space ($h_i(\mathbf{x})=1$ iff $\mathbf{x}$ in the $i$th cluster)</li>
<li>$k$-means algorithm starts with $k$ centroids $\mathbf{\mu}^{(1)}, &hellip;, \mathbf{\mu}^{(k)}$ chosen somewhat arbitrarily (probably make them evenly spread in some sense).  Then alternate steps A and B, where step A assigns each datapoint to its nearest centroid, then step B is to replace each centroid with the mean of all datapoints currently assigned to it.  Do this until it seems to be converging</li>
<li>Easy to come up with mathematical measures of how good a clustering is, hard to say if the clusterings represent anything useful in the real world</li>
<li>Shortcoming of this one-hot approach: think of clustering red cars, blue cars, red trucks, blue trucks.  Any one-hot coding is going to be unsatisfactory, so can be useful to have sparse representations that are not one-hot</li>
</ul></li>
</ul>

<h3 id="stochastic-gradient-descent">Stochastic Gradient Descent</h3>

<ul>
<li>Cost function used by a ML algorithm often decomposes as a sum over training examples of a per-example loss function, e.g. conditional log-likelihood $J(\theta) = \frac{1}{m} \sum_{i=1}^m L(\mathbf{x}^{(i)}, y^{(i)}, \theta)$, where $L(\mathbf{x}, y, \theta) = -\log(p(y|\mathbf{x}, \theta))$</li>
<li>To minimise this, we need to compute the gradient $\nabla_{\theta} J(\theta) = \frac{1}{m} \sum_{i=1}^m \nabla_{\theta} L(\mathbf{x}^{(i)}, y^{(i)}, \theta)$, which is going to be expensive if you have a large training set</li>
<li>From the above form, the gradient is an expectation, so you can approximate it by sampling.  At each step, sample a <em>minibatch</em> $\mathbb{B} = (\mathbf{x}^{(1)}, &hellip;, \mathbf{x}^{(m&rsquo;)})$ where $m&rsquo;$ is small (say, between 1 and 100).  Approximate the gradient by computing with these samples only, then walk downhill in the direction your approximation suggested</li>
</ul>

<h3 id="building-a-machine-learning-algorithm">Building a Machine Learning Algorithm</h3>

<ul>
<li>ML algorithms typically involve a dataset, a cost function, an optimization procedure, and a model</li>
<li>Can tweak cost function and optimization procedure to get new algorithms, e.g. add weight decay to cost function in linear regression</li>
</ul>

<h3 id="challenges-motivating-deep-learning">Challenges Motivating Deep Learning</h3>

<ul>
<li>Development of DL motivated by failure of traditional algorithms on speech recognition and object recognition</li>
<li>Curse of dimensionality: many problems become difficult when number of dimensions is high

<ul>
<li>Need lots of data to get a reasonable covering of possibly values of $\mathbf{x}$</li>
<li>But lots of data means lots of computation</li>
</ul></li>
<li>We&rsquo;ve talked about priors formally in terms of prior distribution $p(\theta)$.  More generally, though, prioirs are any prior assumptions that affect the output model, e.g. choice of model family is itself a prior.  Generally these are hard to quantify with probability</li>
<li>E.g. <em>smoothness prior</em>: we assume that the functions we learn should be smooth / locally constant.  This is implicit in many simpler algorithms (kNN, kernel methods, decision trees)

<ul>
<li>Smoothness prior pretty much ties you down to a slicing of the input space, where the resulting number of regions is at most the number of trainng samples.  That&rsquo;s somewhat restrictive</li>
<li>E.g. how to learn a chessboard?  With local constancy, need a training sample on each square.  (Obviously can preprocess the input space, but the spirit of the question is one where you don&rsquo;t know you&rsquo;re learning a chessboard ahead of time)</li>
</ul></li>
<li>This can be solved, e.g. you can define $O(2^k)$ regions with $O(k)$ samples, so long as you introduce some dependencies between the regions through additional assumptions about the underlying data generating distribution, e.g. for chessboard introduce the assumption that target function is periodic (which is equivalent to preprocessing the input space)

<ul>
<li>Doing things like specifying that the target function is periodic is a massive assumption to feed in to your model</li>
<li>You don&rsquo;t have to feed specific assumptions in like this for DL.  DL itself already makes an assumption that your data is generated by a <em>composition of factors</em>, so presumably at some layer it should learn the periodicity itself and take that into account for future layers</li>
</ul></li>
<li>ML people use the term <em>manifold</em> fairly loosely, roughly related to the mathematical one.  One difference is that the dimension of the manifold doesn&rsquo;t have to be constant (e.g. a figure 8 is a &ldquo;manifold&rdquo;, mostly 1D except for one point where it is 2D)</li>
<li><em>Manifold learning</em> assume that most of $\mathbb{R}^n$ consists of invalid inputs, and that interesting inputs only occur along various manifolds embedded inside $\mathbb{R}^n$

<ul>
<li><em>Manifold hypothesis</em>: the probability distribution over images, text strings, and sounds that occur in real life is highly concentrated.  An image generated by randomly selecting a black or white pixel at each point is probably not going to look like a human face.<br />

<ul>
<li>So &ldquo;most of $\mathbb{R}^n$ is invalid input&rdquo;</li>
<li>Also need to show that interesting input are on manifolds (rather than just sparse isolated points).  Can imagine this informally, e.g. small perturbations, dimming/brightening an image, rotating, etc.</li>
</ul></li>
<li>Would be very useful if the ML algorithm understood these manifolds and could give coordinates on them.  Will apparently explain how to do this by the end of the book</li>
</ul></li>
</ul>

<h2 id="chapter-6-deep-feedforward-networks">Chapter 6 - Deep Feedforward Networks</h2>

<ul>
<li><em>Deep feedforward networks</em> (DFNs) (or <em>feedforward neural networks</em>, or <em>multilayer perceptrons</em>): will fit into standard framework: learn $y = f^*(\mathbf{x})$ by considering a family $f(\mathbf{x}; \mathbf{\theta})$ and learning the $\mathbf{\theta}$ that gives the best approximation</li>
<li>They are <em>feedforward</em> because information flows completely from $\mathbf{x}$ to $y$ - there is no <em>feedback</em>.  (When there is feedback, this is a recurrent neural network (RNN))</li>
<li>Convolutional neural networks (CNNs), popular in object recognition, are a special kind of DFN.  DFNs are also a conceptual stepping stone towards RNNs (which are popular in natural language tasks)</li>
<li>Feedforwarded networks have an associated DAG, nodes in the DAG are functions to evaluate and edges represent composition

<ul>
<li>Have <em>layers</em>.  First one you encounter when traersing the DAG is the input layer, last one is the output layer.  Layers in the middle are called hidden layers</li>
<li>Overall length to the output layer is called the <em>depth</em></li>
<li>Each hidden layer is typically vector valued, the dimensionality of these hidden layers is called the <em>width</em> of the layer</li>
<li>Can think of each layer as a vector-to-vector function, or as composed of a number of <em>units</em> which are vector-to-scalar functions.  The latter point of view can be pushed in to a rough anology with brains</li>
</ul></li>
<li>Motivation: Consider trying to use a linear model (e.g. linear regression, logistic regression) in a case where there is a non-linear relationship between the input and the target.  Previously we would try to do some preprocessing with a function $\phi$

<ul>
<li>This could be hand-crafted, but any attempt is going to be very specialized to the problem/domain</li>
<li>Could use a very generic feature mapping like the RBF kernel, but this often does not generalize well (this optimises for finding a locally smooth solution, rather than digging in to the problem at hand)</li>
<li>In DL you learn $\phi$.  Have $y = f(\mathbf{x};\mathbf{\theta}, \mathbf{w}) = \phi(\mathbf{x};\mathbf{\theta})^T \mathbf{w}$.  Have parameters $\mathbf{\theta}$ used to learn $\phi$ from a family of functions, and $\mathbf{w}$ for the actual linear regression part.  This is a feedforward neural network with one hidden layer.  This makes the optimisation problem harder (no longer convex) but we can use numerical methods</li>
</ul></li>
</ul>

<h3 id="learning-xor">Learning XOR</h3>

<ul>
<li>The target function is $y = f^*(\mathbf{x})$ where $\mathbf{x}$ is a binary pair and $f^*$ is <code>XOR</code>.  Our model provides a function $y = f(\mathbf{x}; \theta)$ and our learning algorithm will adopt the parameters to give a good approximation to $f^*$.

<ul>
<li>There is no statistical generalisation here, we can just try to get a perfect score on the training set $X = ((0,0), (0,1), (1, 0), (1, 1))$</li>
<li>To make things simple, we model it as a regression problem with MSE loss function, so $J(\mathbf{\theta}) = \frac{1}{4} \sum_{\mathbf{x} \in X} (f^*(\mathbf{x})-f(\mathbf{x};\mathbf{\theta}))^2$</li>
<li>If we choose a linear model $f(\mathbf{x}; \mathbf{w}, b) = \mathbf{x}^T \mathbf{w} + b$ and minimise in the usual way with the normal equations, we end up with $\mathbf{w}=\mathbf{0}$ and $b=\frac{1}{2}$.  Clearly a linear function cannot learn <code>XOR</code></li>
<li>Instead we want to transform feature space to a different space where a linear model becomes appropriate.  Introduce a single hidden layer $f^{(1)}(\mathbf{z}; \mathbf{W}, \mathbf{c})$, keep the linear regression output layer $y = f^{(2)}(\mathbf{h}; \mathbf{w}, b)$, so the whole model is $f(\mathbf{x}; \mathbf{W}, \mathbf{c}, \mathbf{w}, b) = f^{(2)}(f^{(1)}(\mathbf{x})))$</li>
<li>Clearly $f^{(1)}$ must be non-linear here.  Most neural networks use an affine transformation controlled by learning parameters, followed by a fixed non-linear activation function.  We do that here, with $\mathbf{h} = g(\mathbf{W}^T\mathbf{x}+\mathbf{c})$.  The activation function $g$ is typically chosen to be a function that is applied element-wise (i.e. $h_i = g((\mathbf{W}^t\mathbf{x})_{i} + c_i)$).  The <em>rectified linear unit</em> (ReLU) $g(z) = \max(0, z)$ is a common choice which we use here</li>
<li>The whole thing is now $f(\mathbf{x}; \mathbf{W}, \mathbf{c}, \mathbf{w}, b) = \mathbf{w}^T\max(\mathbf{0}, \mathbf{W}^T\mathbf{x}+\mathbf{c}) + b$.  At this point, can find a solution by inspection.  Of course, we generally wouldn&rsquo;t be able to see a solution immediately, so we would have to do the gradient optimisation algorithm and get some approximation</li>
</ul></li>
</ul>

<h3 id="gradient-based-learning">Gradient-Based Learning</h3>

<ul>
<li>Biggest difference between feedforward neural networks and previous examples is the non-convexity of the loss function.  This means a numerical gradient method is a must</li>
<li>Moreover, size of the training set means that we have to use <em>stochastic</em> gradient descent.  Stochastic gradient descent doesn&rsquo;t have any guarantees on convergence to minimum</li>
<li>Computing the gradient looks like it might be expensive, but we&rsquo;ll cover the <em>backpropagation algorithm</em> for that</li>
<li>Use regularization techniques as well.  E.g. weight decay can be applied pretty much verbatim, there are also other ones we&rsquo;ll cover later</li>
<li>Cost function:

<ul>
<li>In most cases, we have a parametric set-up and can calculate $p(\mathbf{y}|\mathbf{x}, \mathbf{\theta})$, and we can use maximum likelihood

<ul>
<li>As usual, this is $$J(\mathbf{\theta}) = -\mathbb{E}_{\mathbf{x}, \mathbf{y} \sim \widehat{p}_{\text{data}}} \left(\log p_{\text{model}}(\mathbf{y}|\mathbf{x})\right)$$</li>
<li>The exact form depends on $p_\text{model}(\mathbf{y}|\mathbf{x},\theta)$.  If we work with $p_{\text{model}}(y|\mathbf{x},\mathbf{\theta}) = \mathcal{N}(y;f(\mathbf{x},\mathbf{\theta}),\mathbf{I})$ then we get mean squared error cost again</li>
<li>Maximum likelihood is good, because you don&rsquo;t have to ad hoc construct a cost function, it just comes out of your model</li>
<li>Gradient of cost function must be &ldquo;large and predictable&rdquo; enough.  There is a risk for some of the units in the network to flatten the gradient, and it&rsquo;s difficult to be sure you&rsquo;re optimising correctly if the gradient is flat.  Log likelihoods help as they unflatten by undoing the exponentation in some units.  Authors refer to this flattening as <em>saturation</em></li>
<li>Usually the cross-entropy function in MLE does not actually have a minimum in these cases (think of the exponentials in ReLUs etc.) so some regularisation is needed to stop things getting carried away</li>
</ul></li>
<li>In some cases, we only need to predict some statistic of $\mathbf{y}$ conditional on $\mathbf{x}$, e.g. have a predictor $f(\mathbf{x}; \mathbf{\theta})$ that we want to use to predict the mean of $\mathbf{y}$

<ul>
<li>Using calculus of variations, we can find the solution to $f^* = \text{argmin}_f \mathbb{E}_{\mathbf{x}, \mathbf{y} \sim \widehat{p}_\text{data}} (\mathbf{y}-f(\mathbf{x}))^2$ is $f^*(x) = \mathbb{E}_{\mathbf{y} \sim \widehat{p}_{\text{data}}(\mathbf{y}|\mathbf{x})} [\mathbf{y}]$.  To make sense of this, think of having a really large dataset where you have a bunch of different predictions for $\mathbf{x}$; this is saying that your best bet is to predict the mean</li>
<li>On the other hand, if we instead minimise the $L^1$ norm (<em>mean absolute error</em>), we recover a function that predicts the <em>median</em> value $\mathbf{y}$ for each $\mathbf{x}$</li>
<li>These don&rsquo;t give great results apparently, seems to be common to predict the full distribution even if you just want the mean at the end of the day</li>
</ul></li>
</ul></li>
<li>Output units:

<ul>
<li>Choice of output unit is coupled to cost function, e.g. if you&rsquo;re using cross-entropy then the output units determine the form of the cross-entropy function</li>
<li>Assume hidden layers give $\mathbf{h} = f(\mathbf{x}; \mathbf{\theta})$</li>
<li>For a linear output, can use a linear unit $\widehat{\mathbf{y}} = \mathbf{W}^{\intercal}\mathbf{h}+\mathbf{b}$

<ul>
<li>Typically assume Gaussian noise as well, $p(\mathbf{y}|\mathbf{x}) = \mathcal{N}(\mathbf{y};\widehat{\mathbf{y}},\mathbf{I})$, and then we&rsquo;re back in the familiar maximise log-likelihood / minimise MSE situation</li>
<li>Could also try to learn the covariance matrix, but since it has to remain positive definite the optimisation is a bit more tricky.  We&rsquo;ll see other units used to parametrise covariance</li>
</ul></li>
<li>Suppose you had binary output instead.

<ul>
<li>MLE approach would be to define a Bernoulli distribution for $\mathbf{y}$ conditional on $\mathbf{x}$, then you just have to find the most likely value of $P(y=1|\mathbf{x})$.  Because it&rsquo;s simpler to work with logs, we instead find the most likely value of the logarithm.  We work with an unnormalized version $\tilde{P}$ of $P$ (can normalise later), so must find the most likely value of $\log \tilde{P}(y=1|\mathbf{x})$</li>
<li>We have $z = \mathbf{w}^T\mathbf{h}+b$, and we assume that the probability takes the form $\log \tilde{P}(y|\mathbf{x}) = yz$.  Then easy to show that $P(y|\mathbf{x}) = \sigma((2y-1)z)$, where $\sigma(x) = 1/(1+e^{-x})$</li>
<li>This sort of thing is quite common in ML.  The variables fed in to these logistic functions ($z$ here) are called <em>logits</em></li>
<li>Loss function is $J(\mathbf{\theta}) = - \log P(y|\mathbf{x}) = \zeta((1-2y)z)$, where $\zeta(x) = \log(1+\exp(x))$ is the softplus function.  This doesn&rsquo;t saturate during gradient-based optimisation, unlike what comes out in direct MSE (where you are working directly with $\sigma$)</li>
</ul></li>
<li>Now suppose you had a multinoulli output

<ul>
<li>Need to produce $\mathbf{\widehat{y}}$, with $\widehat{y}_i = P(y=i|\mathbf{x})$.  We have a linear layer predicting $\mathbf{z} = \mathbf{W}^{\intercal}\mathbf{h}+\mathbf{b}$, and we interpret these as unnormalized log probabilities $z_i = \log \tilde{P}(y=i|\mathbf{x})$</li>
<li>Then apply the softmax function $\text{softmax}(\mathbf{z})_i = \frac{\exp(z_i)}{\sum_j \exp(z_j)}$</li>
<li>This leads to log-likelihood $\log \text{softmax}(\mathbf{z})_i = z_i - \log \sum_j \exp(z_j)$.  The second term is roughly the max of the $z_k$, so when we are tweaking the $z_i$ to maximise the log likelihood, we are pushing $z_i$ up and pushing the most loudest incorrect prediction $z_k$ down</li>
<li>Similar point about log-likelihood working well, but any objective function which does not use a $\log$ to undo the $\exp$ in the softmax will not work well</li>
<li>Softmax itself can also be translated to make it more numerically stable $\text{softmax}(\mathbf{z}) = \text{softmax}(\mathbf{z}-\text{max}_i(z_i))$</li>
<li>Softmax is a misleading name, it&rsquo;s more of a softargmax, i.e. it&rsquo;s a soft version of something that picks the <em>index</em> which maximises, it does not pick the maxiumum <em>value</em></li>
</ul></li>
<li>In general, we can view the hidden layers of the neural network as outputting $\mathbf{\omega} = f(\mathbf{x}; \theta)$, and we regard this as the parameters for the distribution at our output layer.  Then we have $p(y|\mathbf{\omega})$, and MLE says we should use $-\log p(y|\mathbf{\omega}(\mathbf{x}))$ as our cost function

<ul>
<li>Consider learning variance of $\mathbf{y}$ given $\mathbf{x}$.  This is easy if $\sigma^2$ is constant (indeed, the MLE is just the empirical variance),  But if we don&rsquo;t make that assumption, we can just say that variance of $\mathbf{y}$ is something controlled by $\mathbf{\omega} = f(\mathbf{x};\mathbf{\theta})$.</li>
<li>If $\sigma$ does not depend on $\mathbf{x}$ (homoscedastic case), we can just include $\sigma$ as an additional term in $\mathbf{\omega}$, nothing to do with the previous layers going on</li>
<li>If $\sigma$ does depend on $\mathbf{x}$ (heteroscedastic case), then $\sigma$ is actually output by the hidden layers</li>
<li>When actually learning $\sigma$, it&rsquo;s probably helpful to rephrase it in terms of learning the precision matrix instead</li>
<li>We have a constraint that covariance (or precision) matrix be positive definite.  If we assume it&rsquo;s diagonal, then we just need the diagonal entries to be positive, which we can ensure by inserting a softplus function.  Non-diagonal covariance matrices are rare.  (Presumably you have tried to decorrelate features before starting any of this, but maybe this is more complicated in the heteroscedastic case?)</li>
<li>For multimodel regression (predict real values from a multimodal $p(\mathbf{y}|\mathbf{x})$), popular to use Gaussian mixtures for the output, $p(\mathbf{y}|\mathbf{x}) = \sum_{i=1}^n p(c=i|\mathbf{x}) \mathcal{N}(\mathbf{y}; \mathbf{\mu}^{(i)}(\mathbf{x}), \mathbf{\Sigma}^{(i)}(\mathbf{x}))$.  A neural network with Gaussian mixture output is called a <em>mixture density network</em></li>
<li>There is a bunch of stuff to learn here: a multinoulli distribution $p(c|\mathbf{x})$, a matrix of means, and a tensor of covariances (3D in theory, though only a 2D tensor if you impose diagonality).  Learning the latter two is complicated by having to consider which Gaussian your sample came from</li>
<li>Potential for numerical instability, <em>clip gradients</em> might help</li>
<li>Gaussian mixture models are effective in e.g. generative models of speech</li>
</ul></li>
</ul></li>
</ul>

<h3 id="hidden-units">Hidden Units</h3>

<ul>
<li>Hidden units are often not differentiable at a small number of points.  Inherent numerical approximation, the isolatedness, and the fact that they usuall have a left and right derivative means that this isn&rsquo;t really a problem when working with the gradient</li>
<li>Use the general notation: hidden units take input $\mathbf{x}$, compute affine tranformations $\mathbf{z} = \mathbf{W}^T\mathbf{x} + \mathbf{v}$, and then apply some non-linear function elementwise to $\mathbf{z}$</li>
<li>ReLU use the activation function $g(z) = \max(0, z)$.  One drawback is they cannot learn (via gradient-based methdos) on examples for which their activation is zero

<ul>
<li>Generalize to $g(z, \mathbf{\alpha})_i = \max(0, z_i) + \alpha_i \min(0, z_i)$ to capture some this stuff</li>
<li>Setting $\alpha_i = -1$ gives <em>absolute value retification</em>.  This is used in object recognition, where you want to reflect (lol) invariance to polarity reversal</li>
<li><em>Leaky ReLU</em> sets $\alpha_i = 0.01$ or something else small</li>
<li><em>Pamarametric ReLU</em> learns $\mathbf{\alpha}$</li>
<li><em>Maxout units</em> are another generalisation, $g(z)_i = \max_{j \in \mathbb{G}^{(i)}} z_j$, where $\mathbb{G}^{(i)}$ is the $i$th block when we split all the indices into blocks of $k$.  There&rsquo;s a fairly long discussion about this but I think it&rsquo;s just because the author invented them :)</li>
</ul></li>
<li>Can use either the logistic sigmoid $g(z) = \sigma(z)$ or the hyperbolic tangent $g(z) = \tanh(z)$ as activation functions.  (Note $\tanh(z) = 2\sigma(2z)-1$).

<ul>
<li>Sigmoid units are probably not a good idea as a hidden unit.  The gradient saturates away from zero quite dramatically, and the only way we got away with using them as output layers was by using a log-likelihood cost function to undo the exponentiation</li>
<li>If you do want to use something like this as a hidden layer, $\tanh$ is better as it satisfies $\tanh(0)=0$ and generally looks a bit like the identity near $0$, so jamming it on top of a linear transformation doesn&rsquo;t do any weird translation</li>
<li>Sigmoidal activation functions are more useful in recurrent neural networks for giving feedback, guess we&rsquo;ll see some of that later</li>
</ul></li>
<li>There is a lot of flexibility in the hidden units you choose.  E.g. they did an experiment training a feedforward network on MNIST with $\cos$ and got error rate less than one percent</li>
<li>Quote: &ldquo;new hidden unit types that perform roughly comparably to known types are so common as to be uninteresting&rdquo;</li>
<li>Some examples anyway:

<ul>
<li>Can just use the identity function.  Having a purely linear layer feed in to the next layer vs. just having one layer is basically having a matrix that factors vs. allowing general matrices.  Factorisation saves on parameters and often the factorisability assumption is reasonable</li>
<li>Softmax units can be used as hidden units as well.  These act as a switch and apparently are used in memory networks</li>
<li>Radial basis function: $h_i = \exp(-\frac{1}{\sigma_i^2} (\mathbf{W}_{\cdot, i} - \mathbf{x})^2)$.  Becomes active as $\mathbf{x}$ approaches the template $\mathbf{W}_{\cdot, i}$.  Difficult to optimize as it saturates to zero away from the template</li>
<li>Softplus: $g(a) = \zeta(a) = \log(1 + e^{a})$.  This is similar to ReLU, but smooth.  It has been proven to perform worse than ReLU, though, so might as well just use ReLU</li>
<li>Hard tanh: $g(a) = \max(-1, \min(1, a))$.  This is shaped similary to $\tanh$ and the ReLU, but it is bounded</li>
</ul></li>
</ul>

<h3 id="architecture-design">Architecture Design</h3>

<ul>
<li>Basic architectural decisions are: depth of the network?  width of each layer?  activation functions at each layer?</li>
<li>Universal approximation theorem: a feedforward network with a linear output layer and at least one hidden layer with any &ldquo;squashing&rdquo; activation function (e.g. logistic sigmoid) can approximate (to given accuracy $\epsilon&gt;0$) any measurable function $\mathbb{R}^m \to \mathbb{R}^n$ (any $m$ and $n$), provided the network is given enough hidden units.  Can also approximate its derivative.  Can also approximate functions between finite dimensional discrete spaces</li>
<li>This says that there exists a network that is good, it does not mean that your chosen algorithm will be able to <em>learn</em> a network that is good</li>
<li>Also it doesn&rsquo;t say how large the network has to be.  There are some bounds on the size of the approximating networks for specific classes of functions, but the bounds are very large</li>
<li>Interesting trade-off between depth of the network and width of the layers.  Technically you only need depth one to approximate well, but author seems to be saying that if you choose depth smaller than some $d$ (depending on the function you&rsquo;re approximating), then you need a number of units exponential in the number of features (i.e., few very wide layers).  If you choose depth larger than $d$, you need a smaller number of hidden units overall.  Refers to lots of research here</li>
<li>Also refers to lots of research showing the deep neural networks just seem to be empirically better</li>
<li>Most convincing theoretical argument is: you justify the depth by saying that you&rsquo;re making the prior assumption that you&rsquo;re trying to model something which is a composition of many different stages</li>
<li>What kind of neural network (CNN?  RNN?) is also an architecture choice</li>
<li>Don&rsquo;t necessarily have to connect the layers in a chain</li>
<li>Don&rsquo;t necessarily have to have a all units in one layer connected to all units in another layer, e.g. CNNs have a specific pattern of sparse connections between layers</li>
</ul>

<h3 id="backpropagation-and-other-differentiation-algorithms">Backpropagation and Other Differentiation Algorithms</h3>

<ul>
<li>Giving an analytical expression for the gradient is straightforward, but actually computing it is expensive.  Backpropagation gives a cheaper was to evaluate the gradient numerically</li>
<li>Backpropagation only refers to how to compute the gradient, to get a learning algorithm on top of that you will need some gradient-based optimisation, like stochastic gradient descent</li>
<li>Fairly general, can compute $\nabla_\mathbf{x} f(\mathbf{x}, \mathbf{y})$ for an arbitrary function $f$, where $\mathbf{x}$ is a set of variables that we want to differentiate along, and $\mathbf{y}$ are additional variables that we don&rsquo;t care about differentiating along</li>
<li>Talk about neural networks in terms of computation graphs:

<ul>
<li>A node in the graph represents a variable (could be scalar, vector, matrix, tensor, &hellip;)</li>
<li>Have <em>operations</em> which (WLOG) output a single variable (which, as above, could be a vector)</li>
<li>If $y$ is obtained by applying an operation to $x$, draw a directed edge from $x$ to $y$ (possibly annotating to say what the operation actually was)</li>
</ul></li>
<li>Chain rule for vector-valued functions: if $\mathbf{x} \in \mathbb{R}^m$, $\mathbf{y} = g(\mathbf{x}) \in \mathbb{R}^n$, and $z = f(\mathbf{y})$, then $\nabla_\mathbf{x} z = \left(\frac{\partial \mathbf{y}}{\partial \mathbf{x}} \right)^T \nabla_\mathbf{y} z$.  I.e., the gradient wrt $\mathbf{x}$ can be obtained by multiplying a Jacobian by the gradient wrt $\mathbf{y}$</li>
<li>Let $\mathbf{X}$ be a tensor, and write $\nabla_\mathbf{X} z = (\nabla_\mathbf{X} z)_i$ for the tensor gradient, where $i$ is now a multi-index</li>
<li>Chain rule for tensor-valued functions: $\nabla_\mathbf{X} z = \sum_j (\nabla_\mathbf{X} \mathbf{Y}_j) \frac{\partial z}{\partial \mathbf{Y}_j}$.  Here the variable $j$ will be range over all the ways you can index into $\mathbf{Y}$</li>
<li>This gives us the way to write down an algebraic expression for the gradient of a scalar wrt any node (=variable) in the computational graph, but it is expensive to evaluate</li>
<li>Can imagine that chained rule for multiply composed functions leads to lots of repeated terms - need to decide whether to recompute (costs CPU) or store (costs memory)</li>
<li>Given output scalar $u^{(n)}$, want to compute its derivative with wrt input nodes $u^{(1)},&hellip;,u^{(n_i)}$.  Also have remaining nodes $u^{(n_i+1)},&hellip;,u^{(n)}$.  Assume they&rsquo;re ordered in the obvious way.</li>
<li>Forward computation algorithm:

<ul>
<li>Put your input values $(x_1,&hellip;,x_{n_i})$ into the nodes $u^{(1)},&hellip;,u^{(n_i)}$.</li>
<li>Then for $n = n_{i+1}, &hellip;, n$, set $\mathbb{A}^{(i)}$ to be the set of $u^{(j)}$ s.t. $j \in \text{Pa}(u^{(i)})$, then calculate $u^{(i)} = f^{(i)}(\mathbb{A}^{(i)})$.  This notation is bad, but it&rsquo;s just trying to distinguish between a node and the value output from it</li>
</ul></li>
<li>Back propagation, simple case:

<ul>
<li>Assume all variables scalars, want to compute derivative of $u^{(n)}$ wrt $u^{(1)},&hellip;,u^{(n_i)}$.</li>
<li>Run forward propogation to compute the activations</li>
<li>Initialise $\text{gradtable}$, we will set $\text{gradtable}(u^{(i)})$ equal to the compute value of $\frac{\partial u^{(n)}}{\partial u^{(i)}}$</li>
<li>First set $\text{gradtable}(u^{(n)}) = 1$</li>
<li>Then for $j=n-1,&hellip;,1$, compute $\frac{\partial u^{(n)}}{\partial u^{(j)}} = \sum_{i \text{ s.t. } j \in \text{Pa}(u^{(i)})} \frac{\partial u^{(n)}}{\partial u^{(i)}} \frac{\partial u^{(i)}}{\partial u^{(j)}} = \sum_i \text{gradtable}(u^{(i)}) \frac{\partial u^{(i)}}{\partial u^{(j)}}$</li>
<li>When done, return the grad table</li>
</ul></li>
<li>Forward propagation for a typical deep neural network (minibatch size one):

<ul>
<li>Have network of depth $l$, $W^{(1)},&hellip;,W^{(l)}$ the weights, $b^{(1)},&hellip;,b^{(l)}$ the biases, input $\mathbf{x}$, and target $\mathbf{y}$</li>
<li>Initialise $\mathbf{h^{(0)}} = \mathbf{x}$</li>
<li>For $k=1,&hellip;,l$, do $\mathbf{a}^{(k)} = \mathbf{b}^{(k)} + \mathbf{W}^{(k)} \mathbf{h}^{(k-1)}$, then $\mathbf{h}^{(k)} = f(\mathbf{a}^{(k)})$</li>
<li>Set $\widehat{\mathbf{y}} = \mathbf{h}^{(l)}$ and $J = L(\widehat{\mathbf{y}}, \mathbf{y}) + \lambda \Omega(\theta)$ (i.e. cost = loss + regularization)</li>
</ul></li>
<li>Back propagation for a typical deep neural network:

<ul>
<li>Run the above forward propagation</li>
<li>Set $\mathbf{g} = \nabla_\mathbf{\widehat{\mathbf{y}}} J = \nabla_{\widehat{\mathbf{y}}} L(\widehat{\mathbf{y}}, \mathbf{y})$</li>
<li>For $k=l,&hellip;1$:

<ul>
<li>First replace the gradient $\mathbf{g}$ with the derivative of the pre-nonlinearity action $\nabla_{\mathbf{a}^{(k)}} J = \mathbf{g} \cdot f&rsquo;(\mathbf{a}^{(k)})$.</li>
<li>Compute the gradient on the weights and biases: $\nabla_{\mathbf{b}^{(k)}} J = \mathbf{g} + \lambda \nabla_{\mathbf{b}^{(k)}} \Omega(\theta)$, $\nabla_{\mathbf{W}^{(k)}} J = \mathbf{g} \mathbf{h}^{(k-1)T} + \lambda \nabla_{\mathbf{W}^{(k)}} \Omega(\theta)$</li>
<li>Backpropagate one stage: replace $\mathbf{g}$ with $\nabla_{h^{(k-1)}} = {}^{t}\mathbf{W}^{(k)} \mathbf{g}$</li>
</ul></li>
<li>At the end of the day, we have the gradients of $J$ wrt all the $\mathbf{W}^{(k)}$ and the $\mathbf{b}^{(k)}$, so we can do descent</li>
</ul></li>
<li>One approach to backpropagation in general is to take a computation graph and a set of numerical inputs to the graph, and return numerical values giving the gradient at those input values.  Call this <em>symbol-to-number differentiation</em>.  Implementations in Torch and Caffe are like this</li>
<li>Alternative is to take the computational graph and add additional nodes explaining how to compute the derivatives.  This is the approach taken by Theano and Tensorflow.  To actually evaluate, you plug your concrete inputs into the new computational graph.  One advantage of this approach is you can now compute second order derivatives by feeding in the first order derivative graph, etc.</li>
<li>General set-up:

<ul>
<li>Each node in the graph corresponds to a tensor $\mathbf{V}$</li>
<li>For each operation (edge in the computational graph) $\text{op}$, we have a corresponding $\text{bprop}$, defined as follows:  Take any operation $\text{op}$ with inputs $\text{inputs}$, write $\text{op.f}$ for the corresponding mathematical function.  Let $\mathbf{G}$ be the gradient on the output of our operation, and let $\mathbf{X}$ be any input to the operation that we want to compute the gradient of (i.e., we want to step backwards to).  The corresponding backprop $\text{op.bprop}$ satisfies $\text{op.bprop}(\text{inputs}, \mathbf{X}, \mathcal{G}) = \sum_i (\nabla_\mathbf{X} \text{op.f}(\text{inputs})_i)\mathbf{G}_i$</li>
<li>C.f. the chain rule: if $\mathbf{X} \mapsto \mathbf{Y} \mapsto z$, then $\nabla_\mathbf{X} z = \sum_i (\nabla_\mathbf{X} Y_i) \frac{\partial z}{\partial Y_i}$.  So the backprop calculates $\nabla_\mathbf{X} z$, from which we obviously get each $\frac{\partial z}{\partial X_i}$ and hence can recurse</li>
<li>Proceeds to write out the algorithm fully but I think it&rsquo;s clear now</li>
</ul></li>
<li>If the forward graph has $n$ nodes, then the backpropagation is at worst $O(n^2)$ (as it&rsquo;s directed acyclic).  For neural networks, it&rsquo;s going to be $O(n)$ as the graph is a chain.  Of course, the complexity is in terms of number of operations as defined by your graph, if those operations are &ldquo;multiply two very large matrices together&rdquo; then it&rsquo;s still going to be slow</li>
<li>TL;DR of backpropagation is &ldquo;use dynamic programming&rdquo;</li>
<li>History lesson:

<ul>
<li>Initially people used neural network models but they were entirely linear (so they were just ways of representing linear models).  There was pushback (from e.g. Minsky) because these are not even capable of learning <code>XOR</code></li>
<li>People added nonlinearity and worked out how to compute the gradient, including using dynamic programming methods, in the 60s and 70s</li>
<li>Some good applications and backpropagation meant continued popularity (with people like Hinton and Le Cun getting mentions) until the early 90s

<ul>
<li>Other ML techniques took over for a while, until neural networks became fashionable again with deep neural networks in 2006.  Mentions larger datasets and better computers as major factors here</li>
<li>Algorithmically, mentions replacing MSE with cross-entropy as an improvement.  Also a good amount on how replacing sigmoids with ReLUs improved performance - people preferred sigmoids for a while as they are everywhere differentiable and work pretty well on small networks, but ReLUs on larger networks have shown notable performance improvements</li>
</ul></li>
</ul></li>
</ul>

<h2 id="chapter-7-regularization-for-deep-learning">Chapter 7 - Regularization for Deep Learning</h2>

<ul>
<li>General statement: we often find that the best fitting model (in the sense of minimising generalisation error) is a large model (e.g. deep neural network) that has been regularised appropriately</li>
</ul>

<h3 id="parameter-norm-penalties">Parameter Norm Penalties</h3>

<ul>
<li>Parameter norm penalties: $\tilde{J}(\theta; \mathbf{X}, \mathbf{y}) = J(\theta; \mathbf{X}, \mathbf{y}) + \alpha \Omega(\theta)$ for some $\alpha \geq 0$</li>
<li>Typically in DL, $\Omega$ only penalises the weights of the affine transformation, and not the biases (regularising the biases can cause underfitting).  Write $\mathbf{w}$ for the weights, and $\mathbf{\theta}$ for all the parameters (which includes both the weights and the unregularized parameters)</li>
<li>Common parameter penalty is L2 norm, variously called weight decay / ridge regression / Tikhonov regularisation</li>
<li>Local effects of L2: For simplicity consider the case where there are no biases (so $\theta = \mathbf{w})$):

<ul>
<li>Then $\tilde{J}(\mathbf{w}; \mathbf{X}, \mathbf{y}) = \frac{\alpha}{2} \mathbf{w}^{\intercal} \mathbf{w} + J(\mathbf{w}; \mathbf{X}, \mathbf{y})$</li>
<li>And $\nabla_\mathbf{w} \tilde{J}(\mathbf{w}; \mathbf{X}, \mathbf{y}) = \alpha \mathbf{w} + \nabla_\mathbf{w} J(\mathbf{w}; \mathbf{X}, \mathbf{y})$</li>
<li>So we see that gradient descent at rate $\epsilon$ now combines two tasks: lowering the gradient of $J$, and decreasing the weights by a constant multiplicative factor $1-\epsilon \alpha$</li>
</ul></li>
<li>Global effects of L2: take a quadratic approximation of the cost function around the point $\mathbf{w}^*$ where $J(\mathbf{w}; \mathbf{X}, \mathbf{y})$ is minimised.  We find the minimum of the appromximation is $\widetilde{\mathbf{w}} = Q(\Lambda + \alpha \mathbf{I})^{-1}\Lambda Q^{\intercal} \mathbf{w}^*$, where $H = Q\Lambda {}^tQ$ diagonalizes the Hessian ($Q$ being orthogonal, as $H$ is real and symmetric).  If $\alpha=0$ this recovers the orginal minimum $\mathbf{w}^*$, but in general it scales the weights down by $\lambda_i/(\lambda_i + \alpha)$ along the eigenvectors of $H$

<ul>
<li>This makes sense: if an eigenvalue of the Hessian is small, then that means that the gradient does not vary much along the direction of the corrresponding eigenvector.  You want your model to assign a low weight to that direction, otherwise you are overfitting</li>
</ul></li>
<li>Another option is L1 regularisation.  In the simplest no-biases case from above, $\widetilde{J}(\mathbf{w}; \mathbf{X}, \mathbf{y}) = \alpha ||\mathbf{w}||_1 + J(\mathbf{w}; \mathbf{X}, \mathbf{y})$ and $\nabla_\mathbf{w} \widetilde{J}(\mathbf{w}; \mathbf{X} \mathbf{y}) = \alpha \text{sgn}(\mathbf{w}) + \nabla_\mathbf{w} J(\mathbf{w}; \mathbf{X}, \mathbf{y})$

<ul>
<li>Clearly this is quite different to L2 regularisation: now regularisation only depends on the sign of $w_i$, not the actual size of $w_i$</li>
<li>Since the analysis is harder here, further simplify by assuming the Hessian is diagonal (can achieve this by applying PCA to remove all correlation between input features as a preprocessing step)</li>
<li>Take a quadratic approximation again.  Still assuming the Hessian is diagonal, we now get $\widehat{J}(\mathbf{w}) = J(\mathbf{w}) + \sum_i \left(\frac{1}{2} H_{i, i} (w_i - w_i^*)^2 + \alpha |w_i|\right)$

<ul>
<li>This is minimised when $w_i = \text{sgn}(w_i^*) \max\left(|w_i^* - \frac{\alpha}{H_{i,i}}|, 0 \right)$</li>
<li>So if $|w_i^*| H_{i,i}$ is small, then we simply replace that weight by zero.  If it is large, we shorten that weight by distance $\frac{\alpha}{H_{i,i}}$</li>
</ul></li>
<li>L1 regularisation therefore moves to a solution that is more <em>sparse</em>.  This can be used as a <em>feature selection</em> mechanism, e.g. in LASSO (L1 penalty with least-squares cost function)</li>
</ul></li>
<li>We previously saw that L2 regularisation was equivalent to MAP Bayesian inference with a Gaussian prior on the weights.  For L1 regularisation, there is a MAP perspective where the prior is an isotropic Laplacian distribution</li>
</ul>

<h3 id="norm-penalities-as-constrained-optimization">Norm Penalities as Constrained Optimization</h3>

<ul>
<li>Suppose you wanted to impose $\Omega(\theta) \leq k$.  Then you could set up a Lagrangian $\mathcal{L}(\theta, \alpha) = J(\theta) + \alpha(\Omega(\theta)-k)$.  The solution is then $\theta^* = \text{argmin}_\theta \max_{\alpha\geq 0} \mathcal{L}(\theta, \alpha)$</li>
<li>From this, if we can find $\alpha^*$ minimising, we get the same form as before.  We could also use a different optimisation algorithm, e.g. do normal stochastic gradient descent then project back to the nearest point satisfying $\Omega(\theta) \leq k$</li>
</ul>

<h3 id="regularization-and-under-constrained-problems">Regularization and Under-Constrained Problems</h3>

<ul>
<li>Regularisation is sometimes necessary for the problem to be defined.  E.g. in linear regression and PCA, you need to invert $\mathbf{X}^{\intercal} \mathbf{X}$, which might not be possible.  But if you regularise, you are inverting $\mathbf{X}^{\intercal} \mathbf{X} + \alpha \mathbf{I}$, which will be fine for almost all $\alpha$.  This is basically the idea of the Moore&ndash;Penrose pseudoinverse</li>
<li>Another example: logistic regression, where we find $\mathbf{w}$ which gets a perfect score.  Then $2\mathbf{w}$ also gets a perfect score but has higher likelihood.  Regularisation at least stops this pattern continuing at some point</li>
</ul>

<h3 id="dataset-augmentation">Dataset Augmentation</h3>

<ul>
<li>Data augmentation can sometimes help reduce generalisation error as well

<ul>
<li>E.g. in object recognition, augment the data by appyling various translations etc. to the input but keep the class the same, makes the network more robust</li>
<li>Similarly injecting noise - neural networks can be sensitive to noise, and one workaround is train them on artificially noised data</li>
</ul></li>
</ul>

<h3 id="noise-robustness">Noise Robustness</h3>

<ul>
<li>Noise can also be added to the hidden units, will cover this in more details when we look at the dropout algorithm</li>
<li>Consider a linear regression problem with training data $(\mathbf{x}_1, y_1),&hellip;,(\mathbf{x}_n, y_n)$ where we are minimising the MSE.  We use a neural network and include a random perturbation $\mathbf{\epsilon}_\mathbf{W} \sim \mathcal{N}(\mathbf{\epsilon}; \mathbf{0}, \eta \mathbf{I})$ of the weights, and write $\mathbf{y}_{\mathbf{\epsilon}_\mathbf{W}}$ for the perturbed model.

<ul>
<li><strong>TODO</strong> I don&rsquo;t understand this example, maybe reading this <a href="https://www.google.com/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=2&amp;cad=rja&amp;uact=8&amp;ved=2ahUKEwjp6PbXmtfkAhXnRBUIHeQwD7kQFjABegQIBBAC&amp;url=https%3A%2F%2Fpapers.nips.cc%2Fpaper%2F899-simplifying-neural-nets-by-discovering-flat-minima.pdf&amp;usg=AOvVaw2U8yZT7vC8xRDmw4Dn6kWL">paper</a> will help</li>
</ul></li>
<li>The training data probably has mistakes, and if $(\mathbf{x}, y)$ is misclassified then maximising $\log(p(y|\mathbf{x}))$ is harmful.  Adding noise to the labels can help mitigate this, e.g. could say that labels are correct with probability $1-\epsilon$</li>
<li><em>Label smoothing</em> replaces a $k$-output softmax, by replacing the hard 0 and 1 with $\epsilon/(k-1)$ and $1-\epsilon$ respectively.  Direct softmax has convergence problems (softmax itself never actually reaches 0 or 1, so need weight decay to stop it getting carried away during training), but smooth version works better</li>
</ul>

<h3 id="semi-supervised-learning">Semi-supervised Learning</h3>

<ul>
<li><em>Semi-supervised learning</em>: use both unlabelled samples from $P(\mathbf{x})$ and labelled examples from $P(\mathbf{x}, \mathbf{y})$ to predict $P(\mathbf{y}|\mathbf{x})$.</li>
<li>In DL, semi-supervised learning usually refers to learning a representation $\mathbf{h} = f(\mathbf{x})$, so that samples from the same class have a similar representation (then apply a basic classifier on top of that)</li>
<li>Just discussed very briefly here, see <a href="https://www.google.com/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=2&amp;cad=rja&amp;uact=8&amp;ved=2ahUKEwjb0eLrndfkAhWMTcAKHT_jBQsQFjABegQIBhAC&amp;url=http%3A%2F%2Fwww.acad.bg%2Febook%2Fml%2FMITPress-%2520SemiSupervised%2520Learning.pdf&amp;usg=AOvVaw3UO5UovSG0OzG7c8CHQ7Mo">Chapelle et. al</a> for more details</li>
</ul>

<h3 id="multitask-learning">Multitask Learning</h3>

<ul>
<li><em>Multitask learning</em>: improve generalisation by pooling the examples arising out of several tasks

<ul>
<li>E.g. Imagine tasks sharing common input $\mathbf{x}$, we might produce a shared representation $\mathbf{h}^{\text{shared}}$, then specialising in to two different hidden layers $\mathbf{h}^{(1)}$ from which we predict $\mathbf{y}^{(1)}$ and $\mathbf{h}^{(2)}$ from which we predict $\mathbf{y}^{(2)}$</li>
<li>The prior belief this is expressing is &ldquo;among the factors that explain the variations observed in the data associated with the different tasks, some are shared across two or more tasks&rdquo;</li>
</ul></li>
</ul>

<h3 id="early-stopping">Early Stopping</h3>

<ul>
<li>When training a model with large representational capacity (i.e., ability to overfit), we often see the training error decreasing steadily (albeit more slowly) as we train longer, but the validation error (and so probably the test error) might start increasing again after a while</li>
<li>Better to exit early in this case.  Can use the <em>early stopping</em> algorithm: keep a copy of the parameters that had the best validation error, and to exit once we have performed $N$ iterations of the training phase without bettering the validation error

<ul>
<li>Early stopping is nice because it doesn&rsquo;t affect the learning dynamics.  In weight decay, you have to worry about things like whether you have made $\lambda$ to large.  OTOH, early stopping slots in quite nicely to what you&rsquo;re already doing</li>
<li>Early stopping can be used in conjunction with other regularisation techniques, of course</li>
</ul></li>
<li>Early stopping requires a training/validation data split.  Once you&rsquo;ve used early stopping to determine the optimal training time, you can feed the validation data back into the training data

<ul>
<li>One way is to take the new combined data set, and train for as long as early stopping told you to</li>
<li>Another way is to stack it on top of the work you did during early stopping - use what you already had, and chuck the validation data in as well.  Keep going for a while until things seem to stop improving.  This avoid retraining but has obvious downsides as well</li>
</ul></li>
<li>Intuitively, early stopping is a regulariser because of the U-shape of the validation error curve.  Stopping at the trough of the U is limiting the distance we travel during gradient descent, so it is a means of regularising our parameters</li>
<li>For a simple linear model with quadratic error function and simple gradient descent, early stopping is equivalent to L2 regularisation:

<ul>
<li>Assume for simplicity the only parameters $\mathbf{\theta}$ are the weights $\mathbf{w}$</li>
<li>Take a quadratic approximation $\widehat{J}$ of the cost function $J$ in a neighbourhood of the empirically optimal point $\mathbf{w}^*$, so $\widehat{J}(\mathbf{w}) = J(\mathbf{w}^*) + \frac{1}{2} (\mathbf{w}-\mathbf{w}^*)^{\intercal}H(\mathbf{w}-\mathbf{w}^*)$</li>
<li>Then $\nabla_\mathbf{W} \widehat{J}(\mathbf{w}) = H(\mathbf{w}-\mathbf{w}^*)$</li>
<li>Write $(\mathbf{w}^{(\tau)})_{\tau \geq 1}$ for the points we step on during gradient descent.  Then it&rsquo;s easy to see that $\mathbf{w}^{(\tau)}-\mathbf{w}^* = (\mathbf{I}-\epsilon H)(\mathbf{w}^{(\tau-1)} - \mathbf{w}^*)$</li>
<li>Diagonalising $H = Q\Lambda Q^{\intercal}$ with $Q$ orthonormal, we get $Q^{\intercal} (\mathbf{w}^{\tau} - \mathbf{w}^*) = (\mathbf{I}-\epsilon \Lambda) Q^{\intercal}(\mathbf{w}^{(\tau-1)} - \mathbf{w}^*)$, and hence $Q^{\intercal}w^{(\tau)} = (\mathbf{I} - (\mathbf{I}-\epsilon \Lambda)^\tau) Q^{\intercal} w^* + (\mathbf{I}-\epsilon \Lambda)^\tau \mathbf{w}^{(0)}$</li>
<li>Assume $\mathbf{w}^{(0)}=0$ for simplicity, and assume that $\epsilon$ is small enough so that $|1-\epsilon \lambda_i| &lt; 1$, so $Q^{\intercal}\mathbf{w}^{(\tau)} = (\mathbf{I} - (\mathbf{I} - \epsilon \Lambda)^\tau) Q^{\intercal} \mathbf{w}^*$</li>
<li>C.f. what we saw in L2 regularization, $\widetilde{\mathbf{w}} = Q(\Lambda + \alpha \mathbf{I})^{-1} \Lambda Q^{\intercal} \mathbf{w}^*$.</li>
<li>Matching these up, the rough relation between these parameters is $\tau \approx 1/(\epsilon \alpha)$</li>
<li>The big advantage early stopping has over weight decay is that early stopping <em>chooses</em> $\tau$, so whilst the approaches are roughly equivalent, there&rsquo;s no ad hoc choice of $\alpha$ but rather an algorithm for choosing $\tau$</li>
</ul></li>
</ul>

<h3 id="parameter-tying-and-parameter-sharing">Parameter Tying and Parameter Sharing</h3>

<ul>
<li>Techniques like L2 regularisation and early stopping express a prior about where the parameters should like (near the origin, near where you started, etc.).  In some situations, though, our prior might not be a specific region for where the parameters should live, but instead something like a relationship between weights</li>
<li>Can imagine two classifiers with different input distributions and different output classes, but are doing basically the same job so might expect that the two models have similar weight vectors once trained optimally.

<ul>
<li>One way to express this prior (weights of my two different classifiers are related) is to train one then then regularize the second towards the weights of the first</li>
<li>Another way is to literally force sets of parameters to be equal when training, this is <em>parameter sharing</em>, which saves on some memory at least.  Parameter sharing is widely used in CNNs (will cover later)</li>
</ul></li>
</ul>

<h3 id="sparse-representations">Sparse Representations</h3>

<ul>
<li>We have already seen how L1 penalisation induces sparse parameterisations, i.e. forces many of the weights to become zero.  In $\mathbf{h&rsquo;} = \mathbf{W}\mathbf{h}$, this is basically making a bunch of entries in $\mathbf{W}$ zero</li>
<li>Sparsity can also be induced in the representation, i.e. making a bunch of entries of $\mathbf{h}$ be zero</li>
<li><em>Sparse representations</em> can be achieved in exactly the same way as we got sparse parameterisations: add a penalty (this time for the representation) to the cost function, $\widetilde{J}(\mathbf{\theta}; \mathbf{X}, \mathbf{y}) = J(\mathbf{\theta}; \mathbf{X}, \mathbf{y}) + \alpha \Omega(\mathbf{h})$.  Can use $\Omega$ an L1 penalty similar to before to induce sparsity</li>
</ul>

<h3 id="bagging-and-other-ensemble-methods">Bagging and Other Ensemble Methods</h3>

<ul>
<li>Bagging (boostrap aggregating) is a technique for reducing generalisation error by combining several models, by having those models vote on the output for a test example.  This is an example of <em>model averaging</em>, and fits within <em>ensemble methods</em></li>
<li>Intuition for model averaging: consider training $k$ regression models, and write $\epsilon_i$ for the errors they make.  Assume the errors are mean zero, variance $v$, and have covariance $\mathbb{E}[\epsilon_i \epsilon_j] = c$.  If you average the predictions, then the expected squared error is $\mathbb{E}\left[\left(\frac{1}{k} \sum_i \epsilon\right)^2\right] = \frac{1}{k}(v + (k-1)c)$

<ul>
<li>If $c=v$ (the errors are perfectly correlated) then the averaging has no effect</li>
<li>If $c=0$ (the errors are perfectly uncorrelated) then we have improved the error by a factor of $k$</li>
</ul></li>
<li>Different ensemble methods construct the models in different ways; bagging is one way</li>
<li>In bagging, you construct $k$ different datasets by sampling with replacement from the original dataset, and get $k$ models by training on those $k$ datasets</li>
<li>Model averaging is &ldquo;an extremely powerful and reliable method for reducing generalization error&rdquo;.  Scientific papers usually don&rsquo;t have any averaging, because everyone knows you can make your results better by averaging</li>
<li>Quick mention of <em>boosting</em>, which is an ensemble method which does not aim at regularisation, but rather at increasing the capacity of the input models</li>
</ul>

<h3 id="dropout">Dropout</h3>

<ul>
<li>Bagging involves training multiple models, then evaluating each model on a test sample (then e.g. taking the majority vote).  This is computationally expensive, dropout is a cheaper alternative</li>
<li>Training with dropout:

<ul>
<li>For each training sample, randomly generate a bitmask on the input and hidden units.  The randomness is independent of the sample, typically probability of including an input unit is $0.8$, and $0.5$ for hidden units.  For the units that are off, exclude them from the network (e.g. multiply the unit&rsquo;s output by zero).</li>
<li>During training, we minimise $\mathbb{E}_{\mathbf{\mu}} J(\mathbf{\theta}, \mathbf{\mu})$, where $\mathbf{\mu}$ is the bitmask vector.  This expectation has exponentialy many terms, but we can approximate it by sampling $\mathbf{\mu}$</li>
</ul></li>
<li>For both bagging and dropout, we can make predictions by taking a majority vote.  If we are just classifying, it&rsquo;s clear what to do here.  For predicting an actual probability disstribution, we need to be more specific

<ul>
<li>Bagging: each model $i$ gives a prediction $p^{(i)}(y|\mathbf{x})$, and to combine we just average $\frac{1}{k} \sum_i p^{(i)}(y|\mathbf{x})$</li>
<li>Dropout: this time weight by the probability of the bitmask, $\sum_{\mu} p(\mathbf{\mu}) p(y|\mathbf{x})$.  Obviously we can&rsquo;t compute this full sum, but again approximate by sampling $\mathbf{\mu}$.  Apparently 10-20 samples of $\mathbf{\mu}$ is enough to see good performance improvements</li>
<li>We can also take the <em>geometric</em> mean, $\widetilde{p}(y|\mathbf{x}) = \left(\prod_\mathbf{\mu} p(y|\mathbf{x}, \mathbf{\mu})\right)^{2^{-d}}$, where $d$ is the length of $\mathbf{\mu}$.  We impose that none of the conditional probabilities on the RHS are zero.  Note that the LHS is not normalised</li>
<li>We can approximate $\widetilde{p}(y|\mathbf{x})$ with a single evaluation of a specific model: namely compute $p(y|\mathbf{x})$ in the model where we take the usual neural net and the output of unit $i$ by the probability that unit $i$ is included.  This is the <em>weight scaling inference rule</em></li>
<li>E.g. consider a softmax classifier $P(y|\mathbf{v}) = \text{softmax}(\mathbf{W}^{\intercal}\mathbf{v} + \mathbf{b})_y$, and suppose we do dropout where each unit is included with probability $<sup>1</sup>&frasl;<sub>2</sub>$.  A bit of maths shows that the network with weights $\frac{1}{2}\mathbf{W}$ computes $\widetilde{p}(y|\mathbf{v})$ <em>exactly</em>. This is an example of weight scaling inference, though in general it only gives an approximation</li>
</ul></li>
<li>Dropout is good because it doesn&rsquo;t add much computational cost, and it does not affect the kind of network that can be used (e.g. dropout in RNNs is just fine)</li>
<li>However, since dropout is a regularisation technique, it does reduce the capacity of the model, and hence you are probably offsetting this by using a larger model in the first place.  Dropout does not do well when there are very few labelled training examples</li>
</ul>

<h3 id="adverserial-training">Adverserial Training</h3>

<ul>
<li>Idea is to search for examples that the network will misclassify, i.e. find $\mathbf{x&rsquo;}$ near $\mathbf{x}$ such that the model output at $\mathbf{x&rsquo;}$ is very different to that at $\mathbf{x}$.  We call $\mathbf{x&rsquo;}$ the <em>adverserial example</em></li>
<li>Example of an image $\mathbf{x}$ of a panda (correctly classified).  Replace with $\mathbf{x} + \epsilon \text{sgn}(\nabla_\mathbf{x} J(\mathbf{\theta}, \mathbf{x}, y))$.  This makes no visual difference, but the classifer now classifies it as a gibbon</li>
<li><em>Adverserial training</em>: training on adverserial perturbed examples from the training set</li>
<li>Neural network respond to adversarial training, effectively this forces the neural network function to be locally constant around training datapoints.  This is something that is possible as we are regularising a large function family.  You could not do something like this with logistic regression</li>
</ul>

<h3 id="tangent-distance-tangent-prop-and-manifold-tangent-classifier">Tangent Distance, Tangent Prop and Manifold Tangent Classifier</h3>

<ul>
<li>Recall the manifold hypothesis: assuming that the data lies on a low dimensional manifold in input space (e.g. real world images inside all possible images)</li>
<li><em>Tangent distance algorithim</em>: doing classification, assume that examples on the same manifold share the same category.  Do a nearest neighbours algorithm, but rather than looking at distance between points, look at the distance between their manifolds.  We can approximate this by approximation the manifold by the tangent plane.  This requires upfront knowledge about what the tangent plane is (which you get by e.g. knowing your image is invariant by translation, etc.)</li>
<li><em>Tangent prop</em>: want to force your classifier to be invariant along the manifold along which samples in this class are concentrated, so add a regularisation term $\Omega(f) = \sum_i \left((\nabla_\mathbf{x} f(\mathbf{x}))^{\intercal} \mathbf{v}^{(i)}\right)$, where $\mathbf{v}^{(i)}$ are the manifold tangent vectors at $\mathbf{x}$.  Again this requires upfront knowledge about the manifold and its tangent plane</li>
<li>Compare adverserial training and tangent prop: former encourages the model to be invariant to small perturbations in all directions, latter encourages the model to be invariant to small perturbations in specified directions</li>
<li><em>Manifold tangent classifier</em>: removes the need to specify the tangent directions upfront.  First it uses an autoencoder to learn the manifold structure by unsupervised learning, then uses these learned tangents in tangent prop.  Will cover in more detail later</li>
</ul>

  







  



</article>


      </div>
    </div>
  </main>
  

</body>
</html>
