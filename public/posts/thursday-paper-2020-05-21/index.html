<!DOCTYPE html>
<html lang="en-us">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta name="generator" content="Hugo 0.66.0" />
  
  
  
  <title>
    
    Thursday Paper: Your Classifier Is Secretly An Energy Model | Martin Dickson
    
  </title>
  <link rel="canonical" href="https://mjdickson.dev/posts/thursday-paper-2020-05-21/">
  
  
  
  
  
  
  
  
  <link rel="stylesheet" href="https://mjdickson.dev/css/base.min.f952525d4b9098cc9e2045c0a2498fa05cd925822779ea81381f47b20ac42f5c.css" integrity="sha256-&#43;VJSXUuQmMyeIEXAokmPoFzZJYIneeqBOB9HsgrEL1w=" crossorigin="anonymous">
  
  
</head>
<body>
  <nav class="u-background">
  <div class="u-wrapper">
    <ul class="Banner">
      <li class="Banner-item Banner-item--title">
        <a class="Banner-link u-clickable" href="https://mjdickson.dev/">Martin Dickson</a>
      </li>
      
      <li class="Banner-item">
        <a class="Banner-link u-clickable" href="https://mjdickson.dev/posts/">Posts</a>
      </li>
      
      <li class="Banner-item">
        <a class="Banner-link u-clickable" href="https://mjdickson.dev/research/">Research</a>
      </li>
      
      <li class="Banner-item">
        <a class="Banner-link u-clickable" href="https://mjdickson.dev/about/">About</a>
      </li>
      
    </ul>
  </div>
</nav>
  <script type="text/x-mathjax-config">MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$']]}});</script>
  <script type="text/javascript" async src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML"></script>

  <main>
    <div class="u-wrapper">
      <div class="u-padding">
        

<article>
  <header class="Heading">
  <h2 class="Heading-title">
    <a class="Heading-link u-clickable" href="https://mjdickson.dev/posts/thursday-paper-2020-05-21/" rel="bookmark">Thursday Paper: Your Classifier Is Secretly An Energy Model</a>
  </h2>
  
  <time datetime="2020-05-21T09:08:17&#43;01:00">
    21 May, 2020
  </time>
  
</header>

  <p>I&rsquo;m going to start a regularly series where I try to read a couple of academic papers per week. The plan will be that on Tuesdays I read classic computer science papers (of which I have read embarrassingly few) and on Thursdays I will read more recent machine learning papers (of which I have also read embarrassingly few). I have no formal education in either computer science of machine learning so this might be a disastrously bad idea, but I&rsquo;ll see how it goes.</p>
<p>My Tuesday paper will lean towards distrbuted systems and databases, probably with a lot drawn from <a href="https://dancres.github.io/Pages/">this list</a> initially.</p>
<p>My Thursday paper will mostly be directed by what is being covered at the <a href="https://www.meetup.com/ML-Paper-Club/events/khjgrrybchbcc/">London ML Paper Club Meetup</a>.</p>
<p>Due to time constraints, I&rsquo;m only going to spend an hour on each paper. The goal then is really just to get a high level idea of what the paper is about, without going in to details. If this feels extremely rushed, I&rsquo;ll split the post over a couple of weeks.</p>
<h2 id="introduction">Introduction</h2>
<p>Up today is a paper called <a href="https://arxiv.org/pdf/1912.03263.pdf">You Classifier Is Secretly An Energy Model And You Should Treat It Like One, Grathwohl et al.</a>.</p>
<p>The paper starts by lamenting the drift between generative models and the downstream applications that they originally motivated. Recall that a generative model is something which is aimed at, given the data, learning the true distribution of the data. You might not achieve that, but if you have a good approximation to that distribution, you have a way to benefit downstream applications like semi-supervised learning (I imagine, by generating new data), imputation of data, and calibration of uncertainty. However, the recent trend has apparently been towards direct measurement of how good the generative model itself is (e.g. log-likelihood on held-out validation sets, or even just qualitively assessing if generated data looks like it came from the same distribution as the original data).</p>
<p>So the paper is obviously going to be about generative models with a downstream application in mind. To quote, it&rsquo;s going to use something called <em>Energy Based Models</em> to &ldquo;help realize the potential of generative models on downstream discriminative problems&rdquo;.</p>
<p>The authors claim that using the energy based models along with the classifiers allows the resulting classifiers to perform better with respect to calibration, out-of-distribution detection, and adverserial robustness.</p>
<h2 id="what-is-an-energy-model">What Is An Energy Model?</h2>
<p>The motivation comes from writing a probability density function over $x \in \mathbb{R}^d$ as $p_{\mathbf{\theta}}(\mathbf{x}) = \frac{\exp(-E_{\mathbf{\theta}}(\mathbf{x}))}{Z_{\mathbf{\theta}}}$. The denominator is just the normalizing constant $Z_\mathbf{\theta} = \int_{\mathbf{x}} \exp(-E_{\mathbf{\theta}}(\mathbf{x}))$. The function $E_{\mathbf{\theta}} : \mathbb{R}^d \to \mathbb{R}$ is called the <em>energy function</em>. The paper is extremely terse on why you might bother reparameterising like this. In the meetup, we discussed that learning with an energy function instead of the probability distribution directly is smoothing around the data manifold, so it is like inserting a prior.</p>
<p>Writing your model in this way has downsides, for example computing the denominator is intractible. This makes training difficult. To make some headway, you can express $\frac{\partial \log p_\theta{x}}{\partial \theta}$ in terms of an expectation over the model distribution. You can therefore approximate this using MCMC techniques. More recent approachs use techniques based on something called Stochastic Gradient Langevin Dynamics (SGLD).</p>
<h2 id="what-does-this-have-to-do-with-classifiers">What Does This Have To Do With Classifiers?</h2>
<p>A classification problem with $K$ output classes normally involves a function $f:\mathbb{R}^d \to \mathbb{R}^K$ to produce <em>logits</em>, then an application of a softmax function $p_(y|\mathbf{x}) \propto \exp(f_{\theta}(\mathbf{x})[y])$. In this notation, $f_{\theta}(\mathbf{x})[y]$ denotes the $y$th index of $f_{\theta}(\mathbf{x})$.</p>
<p>We normally convert the logits to probabilities for classes using the softmax function, but another thing we can do is to define an energy based model from these. More specifically, we define $p_{\theta}(\mathbf{x}, y) \propto \exp(f_{\theta}(\mathbf{x})[y])$, and $E_{\theta}(\mathbf{x}, y) = -f_{\theta}(\mathbf{x})[y]$. We can marginalise out $y$ to get a distribution over $\mathbf{x}$, which gives us an energy based model with $E_{\theta}(\mathbf{x}) = -\log \sum_{y} \exp(f_{\theta}(\mathbf{x})[y])$.</p>
<p>As the authors put it, this as &ldquo;a generative model hidden within every standard discriminative model&rdquo;. They call the output of this interpretation a Joint Energy based Model (JEM).</p>
<h2 id="how-can-we-use-this">How Can We Use This?</h2>
<p>Having identified that there is a generative model within your standard discriminative model, you might want to try to train both of them together. There are a few ways that this can be done, but the authors say that the factorisation $\log p_\theta(\mathbf{x}, y) = \log p_\theta(\mathbf{x}) + \log p_\theta(y|\mathbf{x})$, coming straight from the definitions, works best.  They train the classifier part with standard cross-entropy and the energy part with SGLD.</p>
<p>Going back to the introduction of the paper, we should expect that if we train in this way then we get more robust models. This is starting to look plausible.</p>
<h2 id="results">Results</h2>
<p>They first compare a Wide Residual Network architecture trained using their method and compare it to classifiers and generators for the same datasets, and find it performs well as both.</p>
<p>Next, they talk about calibration. A classifier is calibrated if its predictive confidence in a class and its misclassification rate line up as you would hope. This is obviously a desirable feature for real-world applications. Apparently, in recent years classifiers have got better but their calibration has generally got worse. Here, the authors train the same architecture and compare EBM with non-EBM (and no Platt scaling): they find that accuracy is slightly worse but calibration is significantly better.</p>
<p>For out-of-distribution detection, you want to have a function $s_\theta : \mathbb{R}^d \to \mathbb{R}$ which will give you a score for how likely the input came from your distribution. You would expect that the probability distribution itself would be a good function to use here, but somewhat surprisingly there are arguments against this: apparently there are tractible deep models that score poorly in this sense. They instead choose the maximum predicted probability $s_\theta(\mathbf{x}) = \max_y p_\theta(y|\mathbf{x})$. Intuitively, if the classifier is unsure, then maybe that&rsquo;s because the input isn&rsquo;t something from the distribution it&rsquo;s modelling, so in this sense it&rsquo;s similar to calibration. Again, their method performs well on this score. (They also propose a new score for OODD, which I skipped).</p>
<p>For robustness, the authors start by discussing some similarities in the training between their approach and adversarial training, so you would hope that JEMs would have some robustness to adversarial examples. I&rsquo;m going to skip over this part though due to lack of time and knowledge on adversarial attacks.</p>
<h2 id="rounding-off">Rounding Off</h2>
<p>They mention a number of challenges they encountered during training, with lack of feedback on progress and instability. There is quite a lot in the appendix, at least part of which covers this. They close off by saying that these difficulties could likely be overcome with researchers examining the training more, and they think there is value in incorporating the energy model in to the classifier.</p>

  







  



</article>


      </div>
    </div>
  </main>
  

</body>
</html>
